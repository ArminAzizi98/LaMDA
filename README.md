### <div align="center">üëâ LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation<div> 
<div align="center">
  <a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-green"></a> &ensp;
  <a href="https://arxiv.org/pdf/2406.12832v1"><img src="https://img.shields.io/static/v1?label=Paper&message=arXiv&color=red&logo=arxiv"></a> &ensp;

This repository provides the source code of the accepted **EMNLP2024 findings** paper "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation"

  ---
  ***LaMDA: A Memory Efficient Fine-Tuning Approach for LLMs!***

  [**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**](https://arxiv.org/pdf/2406.12832v1)<br>
  [Armin Azizi](https://www.linkedin.com/in/armin-azizi-4560381ba/), 
  [Souvik Kundu](https://ksouvik52.github.io/), 
  [Massoud Pedram](https://mpedram.com/)
  <br>University of Southern California, Intel Labs<br>

</div>

---


## Installation and Setup

To install all dependencies and run the trainer replacement script, use the following commands:

```bash
pip install -r requirements.txt
./replace_trainer.sh
```

## Disclaimer:
This ‚Äúresearch quality code‚Äù is for Non-Commercial purposes and provided by the contributors ‚ÄúAs Is‚Äù without any express or implied warranty of any kind. The organizations (USC or Intel) involved do not own the rights to the data sets used and do not confer any rights to it. The organizations (USC or Intel) do not warrant or assume responsibility for the accuracy or completeness of any information, text, graphics, links or other items within the code. A thorough security review has not been performed on this code. Additionally, this repository may contain components that are out of date or contain known security vulnerabilities.

## Citation & Acknowledgement
````bibtex
@article{azizi2024lamda,
  title={LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation},
  author={Azizi, Seyedarmin and Kundu, Souvik and Pedram, Massoud},
  journal={EMNLP},
  year={2024}
}
````
